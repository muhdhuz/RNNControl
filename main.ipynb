{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main script to train RNN type architectures on audio data with control parameters\n",
    "* Loading/batching from audioDataloader\n",
    "* Control parameter creation/manipulation from paramManager\n",
    "* NN architectures found in utils.architectures\n",
    "* Teacher forcing (using known targets as input) ratio can be changed to <1 to do professor forcing\n",
    "* Training target can be specified by no. of steps, epochs (specify no. of steps> steps required for an epoch or just set it to a very very big number) or a target loss value\n",
    "* training parameters + model state dic + optimizer state dic are saved into a single python dic if savemodel=True \n",
    "\n",
    "To do:\n",
    "* Put training/generation routines into its own script so it can be easily reused/ easier version management - but some variables then need to be imported into that script since these functions rely on many global variables defined here. All functions to be put in utils folder thereafter\n",
    "* Move all training visualizations to Tensorboard\n",
    "* Better integration with the attention training (in another notebook currently)\n",
    "* streamline checkpoint saving - now ok for all conditions except for saving after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "import audioDataloader.dataloader as dataloader\n",
    "from audioDataloader.transforms import mulawnEncode,mulaw,array2tensor,dic2tensor,injectNoise,normalizeDim\n",
    "from paramManager import paramManager\n",
    "from utils.architectures import RNN\n",
    "from utils.myUtils import time_taken,plot_signal\n",
    "#import utils.training as process\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read/write directory parameters\n",
    "#*************************************\n",
    "datadir = 'data/faustData_2019.06.22/dataset'\n",
    "paramdir = 'data/faustData_2019.06.22/dataparams'\n",
    "savemodeldir = 'model'\n",
    "savename = 'model'\n",
    "loadmodelpath = 'model/2019-06-23_19-20-24_model_epoch0_step4400.tar' #shared path to load model, optimizer and TrainingParams\n",
    "\n",
    "# Pytorch parameters\n",
    "#*************************************\n",
    "#--- Note all intervals below are counted in no. of steps. 1 epoch = [len(dataset)//batch_size] steps ---\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "savemodel = True\n",
    "savemodel_interval = 1000 #if 0 (and savemodel=True) will only save model at the end of entire training\n",
    "loadmodel = False #NOTE: if continuing training from previous save remember to also enable loadTrainingParams\n",
    "\n",
    "# Training parameters\n",
    "#*************************************\n",
    "loadTrainingParams = False #if true will ignore the below and load training params from a .pth as specified in loadmodelpath\n",
    "sr = 16000\n",
    "seqLen = 512\n",
    "stride = 1\n",
    "batch_size = 256\n",
    "num_epochs = 1\n",
    "lr = 0.005\n",
    "log_interval = 100 #will print the loss each log_interval \n",
    "max_steps = 10000 #set max_steps > (len(dataset)//batch_size) if training for more than 1 epoch\n",
    "loss_target = 1.0 #alternative to using max_steps, training will end if model achieves this loss target\n",
    "teacher_forcing_ratio = 1.0 #stochastically use either targets or own predictions as input for training. Set to 1 to always use targets\n",
    "\n",
    "#Generation parameters \n",
    "#*************************************\n",
    "max_length = seqLen*3\n",
    "evaluate_interval = 200 #will generate an audio sequence each evaluate_interval for a visual check on the progress\n",
    "\n",
    "#Network parameters\n",
    "#*************************************\n",
    "n_layers = 4\n",
    "hidden_size = 40\n",
    "output_size = 256 #also the no. of mu-law intervals. Both encoding and decoding will depend on this for consistency\n",
    "\n",
    "#Pre-processing parameters\n",
    "#*************************************\n",
    "noise = 0.1 #eg. 0.1 == noise at 10% of signal\n",
    "lowNote =  63 #midi pitch no.\n",
    "hiNote = 75\n",
    "prop = ['instID', 'pressure', 'midiPitch', 'tongue'] #will train on these parameters (need not use all available params)\n",
    "\n",
    "#Define variables that change between runs (do not alter these!)\n",
    "#*************************************\n",
    "list_of_losses = []\n",
    "start_epoch = 0\n",
    "start_step = 0\n",
    "\n",
    "\n",
    "if loadTrainingParams: #will overwrite the above if True\n",
    "    print(\"Loading existing training params...\")\n",
    "    checkpoint = torch.load(loadmodelpath, map_location=device) #map_location in case using cpu\n",
    "    \n",
    "    list_of_losses = checkpoint['loss']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_step = checkpoint['step']\n",
    "    sr = checkpoint['sample_rate']\n",
    "    start = checkpoint['start_time']\n",
    "    datadir = checkpoint['datadir']\n",
    "    paramdir = checkpoint['paramdir']\n",
    "    savemodeldir = checkpoint['savemodeldir']\n",
    "    seqLen = checkpoint['seqLen']\n",
    "    stride = checkpoint['stride']\n",
    "    batch_size = checkpoint['batch_size']\n",
    "    num_epochs = checkpoint['num_epochs'] #can comment out if want to change between runs\n",
    "    lr = checkpoint['lr']\n",
    "    log_interval = checkpoint['log_interval']\n",
    "    #max_steps = checkpoint['max_steps'] #can comment out if want to change between runs\n",
    "    teacher_forcing_ratio = checkpoint['teacher_forcing_ratio']                   \n",
    "    prop = checkpoint['prop']\n",
    "    output_size = checkpoint['output_size']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    n_layers = checkpoint['n_layers']\n",
    "    noise = checkpoint['noise']\n",
    "    lowNote = checkpoint['lowNote']\n",
    "    hiNote = checkpoint['hiNote']\n",
    "    \n",
    "    for key in checkpoint:\n",
    "        if (key != 'model_state_dict') and (key != 'optimizer_state_dict'): \n",
    "            print(key,'=',checkpoint[key])\n",
    "    \n",
    "\n",
    "# Loading warnings below\n",
    "print('*****************')\n",
    "if loadmodel and loadTrainingParams:\n",
    "    print('Will continue training from a previous checkpoint...')\n",
    "if not loadmodel and not loadTrainingParams:\n",
    "    print('Will start training from scratch...')\n",
    "if loadmodel and not loadTrainingParams:\n",
    "    print('Will load existing model weights but not use trained parameters! (Are you sure?)')\n",
    "if not loadmodel and loadTrainingParams:\n",
    "    print('Will initialize new model but use parameters trained from a previous run! (Are you sure?)')\n",
    "print('using',device, 'pytorch',torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the available conditional parameters first\n",
    "#*************************************\n",
    "pm = paramManager.paramManager(datadir, paramdir)\n",
    "datafiles = pm.filenames(datadir)\n",
    "params = pm.getParams(datafiles[0]) \n",
    "print(params.keys())\n",
    "\n",
    "#note midiPitch has to be scaled since the large raw values interfere with the learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset & dataloader\n",
    "#*************************************\n",
    "audiocoding = mulawnEncode(output_size,0,1) #initialize the mu-law encodings\n",
    "targetcoding = mulaw(output_size)\n",
    "rescalePitch = normalizeDim('midiPitch',lowNote,hiNote)\n",
    "#rescaleAmp = normalizeDim('volume',0,0.9)\n",
    "cond_size = len(prop)\n",
    "\n",
    "adataset = dataloader.AudioDataset(sr,seqLen,stride,\n",
    "                                  datadir=datadir,extension='wav',\n",
    "                                  paramdir=paramdir,prop=prop,\n",
    "                                  transform=transform.Compose([injectNoise(weight=noise),audiocoding,array2tensor(torch.FloatTensor)]),\n",
    "                                  param_transform=transform.Compose([rescalePitch,dic2tensor(torch.FloatTensor)]), \n",
    "                                  target_transform=transform.Compose([targetcoding,array2tensor(torch.LongTensor)]))\n",
    "\n",
    "testdataset = dataloader.AudioDataset(sr,seqLen,stride, #for priming during generation\n",
    "                                  datadir=datadir,extension='wav',\n",
    "                                  paramdir=paramdir,prop=prop,\n",
    "                                  transform=transform.Compose([array2tensor(torch.FloatTensor)]), \n",
    "                                  param_transform=transform.Compose([rescalePitch,dic2tensor(torch.FloatTensor)]),\n",
    "                                  target_transform=transform.Compose([array2tensor(torch.LongTensor)]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=adataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to visualize the audio encoding in dataloader\n",
    "\n",
    "print(\"size of dataset is\",len(adataset))\n",
    "print(\"no. of steps per epoch is\",len(adataset)//batch_size)\n",
    "\n",
    "# first pick a section of audio from the dataset\n",
    "samp = adataset.rand_sample()\n",
    "print(\"shape of audio seq is\",samp.shape)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp)), samp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#now inject noise weighted by the signal amplitude (to get roughly constant signal-to-noise ratio across data samples)\n",
    "print(\"audio seq + 10% noise\")\n",
    "samp2 = samp + 0.1 * np.random.uniform(samp.min(), samp.max(), size=len(samp)).reshape(-1,1)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp2)), samp2) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#if required the noise can be fixed as well (constant=True) - see injectNoise() in transforms.py\n",
    "print(\"audio seq + fixed noise\")\n",
    "samp3 = samp + 0.1 * np.random.uniform(-1, 1, size=len(samp)).reshape(-1,1)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp3)), samp3) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#we then encode the sampling to mu-law\n",
    "print(\"audio seq  + 10% noise + mu-law encoding -> this is our input\")\n",
    "samp_coded = audiocoding(samp2)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp_coded)), samp_coded) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to check that the parameter values make sense\n",
    "for step, (inp,target) in enumerate(train_loader): \n",
    "        print(inp[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training cycle\n",
    "#*************************************\n",
    "def train(model,epoch):\n",
    "    model.train() #put in training mode\n",
    "    ave_loss_over_steps = 0\n",
    "    current_loss = 10000 #set at a large initial value \n",
    "    \n",
    "    for step, (inp,target) in enumerate(train_loader):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        hidden = model.init_hidden(batch_size).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(seqLen):\n",
    "            use_teacher_forcing = True if np.random.random() < teacher_forcing_ratio or i==0 else False\n",
    "            #similar to Bengio et al, Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\n",
    "            \n",
    "            if use_teacher_forcing: #feed the target as the next input \n",
    "                outputs, hidden = model(inp[:,i,:],hidden,batch_size)  #input dim: (batch, seq, feature)\n",
    "                loss += criterion(outputs, torch.squeeze(target[:,i],1))\n",
    "                \n",
    "                outputs = nn.functional.log_softmax(outputs,dim=1)\n",
    "                topv, topi = outputs.detach().topk(1)  #choose the strongest activation detach()\n",
    "                predicted_sample = targetcoding.index2float(topi)\n",
    "                \n",
    "            else: #feed its own predictions (output of t-1) as next input\n",
    "                own_inp = inp[:,i,:].clone()\n",
    "                own_inp[:,0] = torch.squeeze(torch.tensor(audiocoding(predicted_sample),\n",
    "                                                          dtype=torch.float,device=device,requires_grad=True),1)\n",
    "                outputs, hidden = model(own_inp,hidden,batch_size)  #input dim: (batch, seq, feature)\n",
    "                loss += criterion(outputs, torch.squeeze(target[:,i],1))\n",
    "                \n",
    "                outputs = nn.functional.log_softmax(outputs,dim=1)\n",
    "                topv, topi = outputs.detach().topk(1)  #choose the strongest activation\n",
    "                predicted_sample = targetcoding.index2float(topi)\n",
    "                           \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ave_loss_per_sample = loss.item()/seqLen   #over each minibatch\n",
    "        ave_loss_over_steps += ave_loss_per_sample\n",
    "        \n",
    "        if (step+1) % log_interval == 0:\n",
    "            current_loss = ave_loss_over_steps/log_interval\n",
    "            print ('{:%Y-%m-%d %H:%M:%S} Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}'.format( \n",
    "                datetime.now(), epoch, num_epochs, step+1, len(adataset)//batch_size, current_loss))\n",
    "            \n",
    "            list_of_losses.append(current_loss)\n",
    "            ave_loss_over_steps = 0\n",
    "            \n",
    "        if (step+1) % evaluate_interval == 0:\n",
    "            result, hs = generate(model,max_length)\n",
    "            plot_signal(result,start=seqLen-1,start_min_max=[-.5,.5])\n",
    "            model.train() #put model back to training mode\n",
    "        \n",
    "        if savemodel_interval != 0 and savemodel:\n",
    "            if (step+1) % savemodel_interval == 0:\n",
    "                torch.save({ #the training parameters that will be saved\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': list_of_losses,\n",
    "                'epoch': start_epoch+epoch,\n",
    "                'step': start_step+step+1,\n",
    "                'sample_rate': sr,\n",
    "                'start_time': start,\n",
    "                'datadir': datadir,\n",
    "                'paramdir': paramdir,\n",
    "                'savemodeldir': savemodeldir,\n",
    "                'seqLen': seqLen,\n",
    "                'stride': stride,\n",
    "                'batch_size': batch_size,\n",
    "                'num_epochs': num_epochs,\n",
    "                'lr': lr,\n",
    "                'log_interval': log_interval,\n",
    "                'max_steps': max_steps,\n",
    "                'teacher_forcing_ratio': teacher_forcing_ratio,                    \n",
    "                'prop': prop,\n",
    "                'output_size': output_size,\n",
    "                'hidden_size': hidden_size,\n",
    "                'n_layers': n_layers,\n",
    "                'noise': noise,\n",
    "                'lowNote': lowNote,\n",
    "                'hiNote': hiNote\n",
    "                },                            \n",
    "                '{}/{}_{}_epoch{}_step{}.tar'.format(\n",
    "                    savemodeldir,start,savename,start_epoch+epoch,start_step+step+1))\n",
    "                \n",
    "                print('model {}_{}_epoch{}_step{}.tar saved'.format(start,savename,start_epoch+epoch,start_step+step+1))\n",
    "\n",
    "        \n",
    "        if step==max_steps or current_loss < loss_target:\n",
    "            torch.save({ #the training parameters that will be saved\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': list_of_losses,\n",
    "                'epoch': start_epoch+epoch,\n",
    "                'step': start_step+step+1,\n",
    "                'sample_rate': sr,\n",
    "                'start_time': start,\n",
    "                'datadir': datadir,\n",
    "                'paramdir': paramdir,\n",
    "                'savemodeldir': savemodeldir,\n",
    "                'seqLen': seqLen,\n",
    "                'stride': stride,\n",
    "                'batch_size': batch_size,\n",
    "                'num_epochs': num_epochs,\n",
    "                'lr': lr,\n",
    "                'log_interval': log_interval,\n",
    "                'max_steps': max_steps,\n",
    "                'teacher_forcing_ratio': teacher_forcing_ratio,                    \n",
    "                'prop': prop,\n",
    "                'output_size': output_size,\n",
    "                'hidden_size': hidden_size,\n",
    "                'n_layers': n_layers,\n",
    "                'noise': noise,\n",
    "                'lowNote': lowNote,\n",
    "                'hiNote': hiNote\n",
    "                },                            \n",
    "            '{}/{}_{}_epoch{}_step{}.tar'.format(\n",
    "                savemodeldir,start,savename,start_epoch+epoch,start_step+step+1))\n",
    "\n",
    "            print('model {}_{}_epoch{}_step{}.tar saved'.format(start,savename,start_epoch+epoch,start_step+step+1))\n",
    "            break\n",
    " \n",
    "\n",
    " \n",
    "def generate(model,max_length,primer=None,paramvect=None,returnHiddenSequence=False):\n",
    "    \n",
    "    hs=[] #list to save hidden states, function returns empty list if returnHiddenSequence=False \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p_inp,target in test_loader:\n",
    "            if primer is not None:\n",
    "                # must clone else primer is changed outside this function\n",
    "                p_inp.data = primer.clone()\n",
    "            seq = np.copy(p_inp[0,:,0])  #extract the original sample\n",
    "            seq_mu = audiocoding(seq)  #mu-law\n",
    "            p_inp[0,:,0] = array2tensor(torch.FloatTensor)(seq_mu) #now we have both the original and mu-lawed samples\n",
    "            break  \n",
    "        generated = seq\n",
    "        p_inp = p_inp.to(device)\n",
    "\n",
    "        hidden = model.init_hidden().to(device)\n",
    "        if returnHiddenSequence :\n",
    "            hs.append(torch.squeeze(hidden).cpu().numpy())\n",
    "        \n",
    "        if p_inp.shape[1] > 1: #if priming with something with len>1\n",
    "            for j in range(p_inp.shape[1]-1):  #build up hidden state\n",
    "                _, hidden = model(p_inp[:,j,:],hidden)\n",
    "        inp = p_inp[:,-1,:]  #feed the last value as the initial value of the actual generation\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            outputs, hidden = model(inp,hidden)\n",
    "            outputs = nn.functional.log_softmax(outputs,dim=1)\n",
    "            topv, topi = outputs.detach().topk(1)  #choose the strongest activation\n",
    "            predicted_sample = targetcoding.index2float(topi)\n",
    "            \n",
    "            generated = np.append(generated,predicted_sample)\n",
    "            \n",
    "            inp[:,0] = torch.from_numpy(audiocoding([predicted_sample])).type(torch.FloatTensor).to(device)\n",
    "            if paramvect is not None:\n",
    "                if callable(paramvect):\n",
    "                    inp[:,1:] = torch.from_numpy(paramvect(i)).type(torch.FloatTensor).to(device)\n",
    "                else:\n",
    "                    inp[:,1:] = torch.from_numpy(paramvect).type(torch.FloatTensor).to(device)\n",
    "                \n",
    "            if returnHiddenSequence :\n",
    "                hs.append(torch.squeeze(hidden).cpu().numpy())\n",
    "                                       \n",
    "        return generated, hs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network, optimizer and objective func\n",
    "#*************************************\n",
    "model = RNN(input_size=1,cond_size=cond_size,hidden_size=hidden_size,output_size=output_size,n_layers=n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "if loadmodel: # load checkpoint if needed\n",
    "    print(\"Loading existing model and optimizer state...\")\n",
    "    model.load_state_dict(torch.load(loadmodelpath, map_location=device)['model_state_dict'])\n",
    "    optimizer.load_state_dict(torch.load(loadmodelpath, map_location=device)['optimizer_state_dict'])\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train!\n",
    "#*************************************\n",
    "if not loadTrainingParams:\n",
    "    start = '{:%Y-%m-%d_%H-%M-%S}'.format(datetime.now())\n",
    "print('{:%Y-%m-%d %H:%M:%S} Starting training at epoch{} step{}...'.format(datetime.now(),start_epoch,start_step))\n",
    "start_time = time.monotonic()\n",
    "for epoch in range(num_epochs):\n",
    "    train(model,epoch)\n",
    "elapsed_time = time.monotonic() - start_time\n",
    "print('Training time taken:',time_taken(elapsed_time))\n",
    "\n",
    "if savemodel_interval == 0 and savemodel: #!NOTE the naming scheme for this saving part in still incorrect (for steps) \n",
    "    torch.save({#the checkpoint_dict here is slightly different from above since it can't access some local variables in training\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': list_of_losses,\n",
    "                            'epoch': start_epoch+epoch,\n",
    "                            'step': start_step,\n",
    "                            'sample_rate': sr,\n",
    "                            'start_time': start,\n",
    "                            'datadir': datadir,\n",
    "                            'paramdir': paramdir,\n",
    "                            'savemodeldir': savemodeldir,\n",
    "                            'seqLen': seqLen,\n",
    "                            'stride': stride,\n",
    "                            'batch_size': batch_size,\n",
    "                            'num_epochs': num_epochs,\n",
    "                            'lr': lr,\n",
    "                            'log_interval': log_interval,\n",
    "                            'max_steps': max_steps,\n",
    "                            'teacher_forcing_ratio': teacher_forcing_ratio,                    \n",
    "                            'prop': prop,\n",
    "                            'output_size': output_size,\n",
    "                            'hidden_size': hidden_size,\n",
    "                            'n_layers': n_layers,\n",
    "                            'noise': noise,\n",
    "                            'lowNote': lowNote,\n",
    "                            'hiNote': hiNote\n",
    "                },                            \n",
    "                    '{}/{}_{}_epoch{}_step{}.tar'.format(\n",
    "                        savemodeldir,start,savename,start_epoch+epoch,start_step))\n",
    "    print('model {}_{}_epoch{}_step{}.tar saved'.format(start,savename,start_epoch+epoch,start_step))\n",
    "    \n",
    "if savemodel_interval == num_epochs: #!NOTE the naming scheme for this saving part in still incorrect (for steps) \n",
    "    torch.save({#the checkpoint_dict here is slightly different from above since it can't access some local variables in training\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': list_of_losses,\n",
    "                            'epoch': start_epoch+epoch,\n",
    "                            'step': start_step,\n",
    "                            'sample_rate': sr,\n",
    "                            'start_time': start,\n",
    "                            'datadir': datadir,\n",
    "                            'paramdir': paramdir,\n",
    "                            'savemodeldir': savemodeldir,\n",
    "                            'seqLen': seqLen,\n",
    "                            'stride': stride,\n",
    "                            'batch_size': batch_size,\n",
    "                            'num_epochs': num_epochs,\n",
    "                            'lr': lr,\n",
    "                            'log_interval': log_interval,\n",
    "                            'max_steps': max_steps,\n",
    "                            'teacher_forcing_ratio': teacher_forcing_ratio,                    \n",
    "                            'prop': prop,\n",
    "                            'output_size': output_size,\n",
    "                            'hidden_size': hidden_size,\n",
    "                            'n_layers': n_layers,\n",
    "                            'noise': noise,\n",
    "                            'lowNote': lowNote,\n",
    "                            'hiNote': hiNote\n",
    "                },                            \n",
    "                    '{}/{}_{}_epoch{}_step{}.tar'.format(\n",
    "                        savemodeldir,start,savename,start_epoch+epoch,start_step))\n",
    "    print('model {}_{}_epoch{}_step{}.tar saved'.format(start,savename,start_epoch+epoch,start_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "#*************************************\n",
    "plt.figure()\n",
    "plt.plot(list_of_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
